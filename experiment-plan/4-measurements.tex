Measurements are needed to gather the parameters to configure the simulation system. As we are interested especially in the inter-unit level challenges of the stream computation, the communication delays are the main focus of our measurements. Also, the intra-unit delays, mainly memory latencies inside the units, are interesting. Blades present the instances of such a network processing units.

The measurements with the blades will be run on top of Linux, which makes the measurements and workload handling easier. However, this also creates overhead in the measurements, and has to be taken into account in the results.

\subsection{Inter-unit Communication Delays}
The processing chain of the inter-unit communication contains several steps, as presented in the previous chapter. We will not able to measure the delays in each of the hardware parts of the communication chain. Thus, the measurements need to be varied to find out the parameters for the underlying statistical behaviour of the phases.

We will measure the total delay in the output phase and the input phase by using the blade hardware counters. CVMX\_CORE\_PERF\_CLK counter returns the clocked cpu cycles. We can determine the communication delay between the blades by sending data packets from blade A to another blade B. We will save the clock cycle once right before the sending the packet from blade A, and again right after the blade B receives the packet. If we send the packets from a blade back to itself through the switch, the absolute timing will be easier to handle. The parameters for the statistical model will be gathered from several repeated measurements, varying the packet size and count each time.

Cavium SDK offers ready-built interface for the use of performance counters. The complete list of per-core counters can be found from the OCTEON Programmer's Guide, p. 6-14.

At the moment, the traffic generation is planned to be done by using external machine. We will create a local area network between the blades and an external packet generator. The packet generator will send packets, using a networking tool (e.g. netcat or mausezahn), to one of the blades, which then forwards the packets to other blades, and finally back to the traffic generator. The communication times could also be measure by using Multi-core Processor Architecture and Communication (MPAC) benchmarking library. Using MPAC, we would measure the loopback time of the packets, starting from the packet generator through one or more of the blades, and then back to the packet generator.

For this setup, the delays in the actual traffic generator might be very large. We will have to measure the latencies of that machine as well.

The passthrough.c example from the Cavium SDK can be used as a base for this experiment. We might also need to implement some simple application that also includes processing and termination of the packets. With a simple OpenEM queue implementations, we can measure the load-balancing overhead when the amount of data transferred exceeds the computation capacity of the blades.

\subsection{Intra-unit Delays}
We will also need to measure the delays, mainly memory latencies of a single blade. This will be done by running some multi-threaded application, implemented for example with OpenMP, and measure it's performance, for example with MPAC.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "experiment-plan-hartikainen"
%%% End:
