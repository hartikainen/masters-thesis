\chapter{Background}
\label{chapter:background}

\todo[inline]{write introduction to the backgroup chapter}
\todo[inline]{rename the background}

\section{Modern Computing}
This chapter introduces the context of this thesis. We present the challenges and reasons that have lead the traditional single threaded computing to concurrent, multi-threaded and parallelized programming, and further to a paradigms called cloud- and mobile-edge computing.

\subsection{Parallel Computing}
\label{section:parallel-computing}
Until the turn of the millenium, the main drivers for speedup in computing were the increase of the CPU clock speeds and cache sizes, as well as the execution optimization. Faster clock speeds meant that more cycles could be run in the same time, whereas the execution optimization enabled more work to be done with the same amount of cycles. Further, the increasing cache sizes kept larger part of the computation close to the CPU cores and away from slow shared memory.~\cite{Sutter:2005:FLiO}

A notable fact about all these drivers is that, while they lead to speedups in parallel, multi-threaded and multi-process computing, they also provide direct speedups in sequential computing. Partly for this reason, until the end of 1990s, the majority of the software applications were single-threaded.~\cite{Sutter:2005:FLiO}

However, in the early 2000s, the CPU performance gains hit the wall. Despite the fact that the transistor density continued growing, following the Moore's Law~\cite{Moore:1998:MooresLaw}, several physical problems limited the development of higher clock speeds. These challenges in the hardware development forced the search of programming models to utilize the performance gains from concurrent, multi-threaded and parallelized programming.~\cite{Sutter:2005:FLiO}

\todo[inline]{KUVA from http://www.gotw.ca/publications/concurrency-ddj.htm}

At the same time, there are clear limitations in the speedup gains that parallel programs can achieve. Amdahl's Law states the rather obvious maxima in the speedup that the program can achieve by scaling the computation over N processors. Suppose that the parallelizable proportion of a program is P (and thus the non-parallelizable proportion equals P-1), then the maximum speedup, as denoted by Amdahl's Law, is:~\cite{Amdahl:1967:VSP}

\begin{equation*}
  \frac{1}{(1-P) + \frac{P}{N}}
\end{equation*}

The implication of Amdahl's Law is that the speedup of a program is always limited by its non-parallezable proportion, and it also brings new challenges to the utilization of the available computing resources.

\section{Dataplane}
\label{sec:dataplane}

\todo[inline]{clean API with a set of coherent libraries and drivers}

\todo[inline]{generic support for many CPUs/NICs}
\todo[inline]{easy to use and understand}
\todo[inline]{documentation}

\todo[inline]{DPDK subscribers/commits}

\todo[inline]{Many of today's dataplane architectures use run to completion model. No cycles can be wasted for scheduling. Applications poll for received packets. Power management?}

\todo[inline]{poll mode drivers}

\todo[inline]{DPDK almost ISA neutral, but has to be done properly}

\todo[inline]{DPDK: Hugepages -> less TLB cache misses}

\todo[inline]{Legacy dataplane? Legacy IPsec?}

\todo[inline]{Protocols, addresses, ports, packet dropping, flow reordering}
\todo[inline]{Reference implementations: Intel DPDK, ODP}

\subsection{Open Event-Machine}

\todo[inline]{vs. Intel DPDK, ODP??}
\todo[inline]{Mapping between fastpath hardware and dataplane application}
\todo[inline]{Event driven parallel programming model}
\todo[inline]{Currently no support for distributed computation}




\section{Cloud Computing}
The development of data center technology and virtualization, together with faster telecommunication networks and proliferation of mobile devices have lead in the adoption of cloud computing. Cloud computing paradigm provides a shared pool of easily configurable and flexible computing resources via convenient, on-demand network access. [3] One of the driving forces for cloud computing has been its promise of economics of scale; cloud computing centers can be built using cheaper hardware, cooling, electricity, network capacity and smaller number of administrators per computer, while at the same time alleviating the problem of inefficient resource usage.

However, to efficiently utilize data center resources, and to provide the required performance guarantees, efficient load balancing mechanisms are needed on various locations in the data center, all the way from single server core utilization up to the efficient network routing.

% [https://www.researchgate.net/post/What_is_the_main_difference_between_Mobile_Edge_Computing_MEC_and_Fog_Computing]
% Both MEC and Fog have the goal to create a platform that provides storage, computing power, and services at the edge of the cloud rather than at the core cloud. I still confused about how to distinguish between them, rather than fog computing is a paradigm for dealing with Internet of Things (IoT).

% " Mobile cloud computing (MCC) and mobile-edge computing
% (MEC) are similar to fog computing. MCC refers to an infrastructure
% in which both the data storage and the data processing happen outside of the
% mobile devices. MEC focus on resource-rich fog servers like cloudlets running
% at the edge of mobile networks. Fog computing distinguishes itself as a more
% generalized computing paradigm especially in the context of Internet of Things."

\section{Mobile-edge Computing}
% scale up vs. scale out
% NIST definiton of cloud computing: http://www.nist.gov/itl/cloud/upload/cloud-def-v15.pdf

% Use cases:
% Device location tracking
% Agumented Reality Content Delivery
% Video Analytics
% Radio Access Network (RAN) -aware Content Optimization
% Distributed Content and DNS caching
% Application-aware Performance Optimization

% Edge: at the base station or at the aggregation point? ETSI specification mentions both!
%   - at base station, the mec would provide minimal latency
%   - at aggregation point, the solution can be more cost effective

% World's first content delivery network (CDN): http://www.saguna.net/news-events/press-releases/saguna-and-akamai-showcase-the-world-s-first-content-delivery-network-operating-from-the-mobile-base-station/?utm_source=Blog?utm_medium=website?utm_campaign=Q2-2015

% PERFORMANCE PARAMETERS!

% !!Service-oriented Architecture (SOA)

% Load balancing problems and congestion on all the levels

% In cloud environment, the computing nodes are heterogeneous
% -> Load balancing is very important, but complex

% CIA model!
% CAP model?
% SNMP protocol?

% ``Move the computation code to the data, not the other way''

% CPU bound vs. IO bound: Florence at al. Energy Aware Load Balancing for Computational Cloud

% ``Cloud computing is the key mantra of todayÃÂ¢ÃÂÃÂs technology
% era. It caters anything as a service. It promotes resource
% sharing through internet. Eventually in cloud computing load
% balancing is a major concern used to allocate the tasks across
% several computers. Major concerns of load balancing are:
% optimal resource utilization, maximize processor throughput,
% minimize task response time, and avoid overloading''

% ``To achieve maximum throughput, in turn to improve the
% overall system performance [3], tasks can be migrated from
% overloaded nodes to lightly loaded nodes. In a cloud
% environment, to enhance the system performance workload is
% divided effectively across the participating nodes[5].There are
% two types of load balancing algorithms [9] existing such as:
% - Static ÃÂ¢ÃÂÃÂ It is non-preemptive. The master distributes the
% workload between the slaves according to their
% performance.Once a host is allocated for a process, it
% cannot be pre-empted at run time. Here workload
% allocation is static.
% - Dynamic ÃÂ¢ÃÂÃÂWork load is distributed at run time
% dynamically. Processes are migrated dynamically from
% over loaded nodes to under loaded nodes in order to
% keep all the nodes busy with balanced load.''

% % [1] M. Kocaoglu, D. Malak, O. Akan, Ã¢ÂÂFundamentals of Green Communications and Computing: Modeling and Simulation,Ã¢ÂÂ Computer, Vol. 45, pp. 40-46, July 2012.
% % [7] K. Abbas, F. Zarafshan, Adznan, B. Jantan, Ã¢ÂÂA New Fuzzy Approach  for Dynamic Load Balancing Algorithm,Ã¢ÂÂ International Journal of Computer Science and Information Security, Vol. 6, pp. 1-5, 2009.
% % [11] George Cybenko, Ã¢ÂÂDynamic Load Balancing for Distributed Memory Multiprocessors,Ã¢ÂÂ Journal of Parallel and Distributed Computing, Vol. 7, pp. 279-301, 1989.
% % [12] A.H.Harp, Ã¢ÂÂProgramming for ParallelismÃ¢ÂÂ, IEEE Computer, Vol. 20, pp. 41 Ã¢ÂÂ 47, 1987.
% %https://portal.etsi.org/Portals/0/TBpages/MEC/Docs/Mobile-edge_Computing_-_Introductory_Technical_White_Paper_V1%2018-09-14.pdf

% ``Load balancing is a core networking solution responsible for distributing incoming traffic among servers hosting the same application content. By balancing application requests across multiple servers, a load balancer prevents any application server from becoming a single point of failure, thus improving overall application availability and responsiveness. For example, when one application server becomes unavailable, the load balancer simply directs all new application requests to other available servers in the pool.

% Load balancers also improve server utilization and maximize availability. Load balancing is the most straightforward method of scaling out an application server infrastructure. As application demand increases, new servers can be easily added to the resource pool, and the load balancer will immediately begin sending traffic to the new server.

% Early generation server load balancers are tried and true solutions for improving the availability and scalability of an organizationÃ¢ÂÂs application infrastructure. Nonetheless, enterprises that persist in using such products run the risk of exposing themselves and their customers to increasingly poor application performance and a seemingly endless stream of application layer security threats.''
% % https://www.citrix.com/glossary/load-balancing.html

% Proactive vs. reactive

% % https://www.usenix.org/legacy/event/hotice11/tech/full_papers/Wang_Richard.pdf
% % [5] S.Lee,R.Panigrahy,V.Prabhakaran,V.Ramasubramanian,K.Talwar,L.Uyeda, U.Wieder,Validatingheuristicsforvirtualmachinesconsolidation,Technical Report,2011.
% % [6] M. Sindelar, R.K. Sitaraman, P. Shenoy, Sharing-aware algorithms for virtual machine colocation, in: Proc. of the 23rd ACM Symposium on Parallelism in AlgorithmsandArchitectures,SPAAÃ¢ÂÂ11,2011.
% % [7] C. Isci, J.E. Hanson, I. Whalley, M. Steinder, J.O. Kephart, Runtime demand estimation for effective dynamic resource management, in: Proc. of the IEEE NetworkOperationsandManagementSymposium,NOMS,2010.
% % https://www.cct.lsu.edu/~xuelin/openflow/sigcomm09-demo-loadbalancer.pdf

% % !! http://www.cs.yale.edu/homes/jf/nox.pdf

% % laja Load-balancing viitteita:
% % Zhang et al. Web server load balancing: A queueing analysis

% Centralized. vs. distributed load balancing
%   - centralized creates overhead, but provides better overview
%   - in centralized design, the network is more fault toleranec
% static vs. dynamic load balancing
% cloud vs. grid. vs. cluster

% eucalyptus greedy first-fit with round-robin vm mapping

% % [15] Lee, R. & Jeng, B. (2011). Load-balancing tactics in cloud . In proc. International Conference on Cyber- Enabled Distributed Computing and Knowledge Discovery (CyberC), IEEE, (pp. 447-454).
% % [16] Wang, S. C., Yan, K. Q., Liao, W. P. & Wang, S. S. (2010). Towards a load balancing in a three- level cloud computing network . Proceedings of 3 rd International Conference on Computer Science and Information Technology (ICCSIT), IEEE, July, 1, 108-113

% % A Comparative Study of Load Balancing Algorithms in Cloud Computing Environment
% % http://arxiv.org/pdf/1403.6918.pdf

% % OpenStack: Toward an Open-Source Solution for Cloud Computing [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.245.229&rep=rep1&type=pdf]


% Unified computing system: https://kemptechnologies.com/server-load-balancing-appliances/loadmaster-for-ucs/overview/

% ``Cloud computing promises to increase the velocity with which applications are deployed, enhance
% modernization, and lower expenses, all at the same time increasing business agility. Cloud Computing is a
% concept that has many computers interconnected through a real time network like internet. Cloud computing
% mainly refers to distributed computing. Cloud computing enables well-situated, on-demand, dynamic and
% reliable utilization of distributed computing assets. The cloud is altering our life by providing users with new
% kinds of services. Users acquire service from a cloud without paying attention to the details. Cloud computing is
% a on demand service in which shared resources work together to perform a task to get the results in minimum
% possible time by distribution of any dataset among all the connected processing units. Cloud computing is also
% referred to refer the network based services which give an illusion of providing a real server hardware but in real
% it is simulated by the software's running on one or more real machines. Such virtual servers do not exist
% physically so they can be scaled up and down at any point of time [1]. Cloud computing is high utility software
% having the ability to change the IT software industry and making the software even more attractive [2]. Hence, It
% helps to accommodate changes in demand and helps any organization in avoiding the capital costs of software
% and hardware [3] [4]''
% % <-Haryani, Jagli: Dynamic Method for Load Balancing in Cloud Computing

% % https://f5.com/resources/white-papers/cloud-balancing-the-evolution-of-global-server-load-balancing

% % Load Balancing in the Cloud Computing Using Virtual Machine Migration: A Review [http://www.ijaiem.org/volume3issue5/IJAIEM-2014-05-31-131.pdf]

% % Load Balancing Cloud Computing: State of Art [http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6249253]

% % Join-Idle-Queue: A novel load balancing algorithm for dynamically scalable web services [http://dl.acm.org/citation.cfm?id=2039718]


% In \ref{churhc et al. On Delivering Embarrassingly Distributed Cloud Services} Church et al. highlight the advantages of small scale data centers over a mega centers.

% % An architecture for Load Balancing Techniques for Fog Computing Environment  [http://csjournals.com/IJCSC/PDF6-2/43.%20Manisha.pdf]

% % Cisco: [http://www.cisco.com/c/en/us/td/docs/solutions/Enterprise/Data_Center/VMDC/ASA_Cluster/ASA_Cluster/ASA_Cluster.html]

% `` Hierarchical Network Layers

% The data center within the VMDC reference architecture is based on the classic multi-layer hierarchical network model. Hierarchical model benefits include scalability, resilience, performance, maintainability, and manageability and its design represents a structured approach to building the infrastructure, allowing for relatively easy expansion in modular increments. Redundant nodes and links at each level insure no single point of failure, while link aggregation can be engineered for optimal bandwidth and performance through the aggregation and core layers. In general, this hierarchical model uses three layers:

% â¢Core LayerâCharacterized by a high degree of redundancy and bandwidth capacity and thus optimized for availability and performance.

% â¢Aggregation LayerâCharacterized by a high degree of high-bandwidth port density capacity and thus optimized for traffic distribution and link fan-out capabilities to access layer switches. Functionally, the nodes in the aggregation layer typically serve as the Layer 2/Layer 3 boundary.

% â¢Access LayerâServes to connect hosts to the infrastructure, providing network access, typically at Layer 2 (L2) (i.e., LANs or VLANs).''

% http://web.stanford.edu/class/ee380/Abstracts/091118-DataCenterSwitch.pdf

% % A Scalable Commodity Data Center Network Architecture,
% % Data Center Switch Architecture in the Age of Merchant Silicon
% % PortLand: A Scalable Fault Tolerant Layer 2 Data Center Network Fabric

% % http://ccr.sigcomm.org/online/files/p63-alfares.pdf


% The goal of load balancing system is to provide optimal resource, e.g. central processing units, memory, or storage, utilization and desired performance metrics, usually maximal throughput, minimizing response time, and avoiding congestion and overloading parts of the system.

% Traditional web server load balancing algorithms are not sufficient for the large scale and distinct structure of so-called service-oriented data centers.

% One on the main requirements for cloud, or fog, computing is efficient load-balancing.
% Telecommunication industry is facing difficult challenges to keep up with more demanding needs of end users and businesses.
% \begin{itemize}
% \item end users need:
%   \item personalization
%   \item performance
%   \item ux
% \item businesses need:
%   \item information from consumers
%   \item easier, secure access to devices
%   \item more flexible provisioning of new services
% \end{itemize}

% Mobile traffic has been growing, and no end to that is in sight. number of sensor expected to grow exponentially. cellular network is becoming uniquitous.
% \begin{itemize}
% \item IOT, huge amount of (wireless) sensors
% \item MACHINE TO MACHINE!
% \item smart phones
% \item streaming video
% \item messaging
% \item P2P
% \end{itemize}

% The IT and Telecommunication Networking evolutions are coming closer to each other.
% -> new possibilities
% -> computation is moving to the network edge, closer to the end user and sensors.

% ``Mobile-edge Computing can be seen as a cloud server running at the edge of a mobile network and performing specific tasks that could not be achieved with traditional network infrastructure. Machine-to-Machine gateway and control functions are one example, but there are many others.''

\section{Fog Computing}
\label{section:fog-computing}
% % http://www.datacenterknowledge.com/archives/2013/08/23/welcome-to-the-fog-a-new-type-of-distributed-computing/
% % Fog Computing and Its Role in the Internet of Things [http://conferences.sigcomm.org/sigcomm/2012/paper/mcc/p13.pdf]
% % https://techradar.cisco.com/technology/fog-computing#prettyPhoto

% fog computing is a paradigm that extends cloud computing to the edge of the network
% in fog computing, intelligent nodes are deployed at strategic locations of the network between the centers and the cloud
% fog computing is similar to the cloud computing, but closer to the end user, the edge, or the ground.
% scalability, reliability, faster response time, reduced cost

% fc enables capabilities that are not possible otherwise
% trains, plane, oil rig generate huge amount of data
% wireless network dont have bandwidth
% too expensive to deliver the data
% sending packet from end user to cloud might be too slow
% manufacturing: decision need to be made in fractions of seconds
% bring processing to the data, not vice versa

% improved reliability:
% network connection to the cloud goes down -> decisions can still be made
% fog computing does not contradict cloud computing, nor replace it, it complements it with much needed compute, storage and networking capabilities, at stratetig loc between sensors and data
% real-time analytics, machine learning algorithms
% analytics and ml algorithms are essential ingredients in many fog computing applications
% data into actionable insights and BI

% huge business opportunity

% % Finding your Way in the Fog: Towards a Comprehensive Definition of Fog Computing [http://delivery.acm.org/10.1145/2680000/2677052/p27-vaquero.pdf?ip=130.233.87.116&id=2677052&acc=ACTIVE%20SERVICE&key=74A0E95D84AAE420%2E1D2200B0A299939C%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&CFID=562969494&CFTOKEN=64137578&__acm__=1448197233_c568466fa204a88d9106d97b20594a8d]
% A definition for fog
% challenges: privacy, volatility, bandwidth, mobility, billions of devices
% According to [https://www.cisco.com/web/about/ac79/docs/innov/IoT_IBSG_0411FINAL.pdf], there will be 50 billion devices connected to the Internet in year 2020.
% This growth is explained by not only the growth in number of mobile devices (mobile phones and tablets, especially in developing countries)
% +!! the proliferation of IoT devices and sensors. (not included in the 50 billion)

% ``Configuring and keeping updated and secure fog networks,
% services and devices is done separately (e.g. routers, servers,
% services and devices are separately managed by different
% people). These tasks are labour intensive and error prone.
% For instance, well-known Internet companies claim a single
% admin handles thousands of machines running a single service
% type. Configuring and maintaining many different types
% of services running on billions of heterogeneous devices will
% only exacerbate our current management problems. The
% fog needs heterogeneous devices and their running services
% to be handled in a more homogeneous manner; ideally fully
% automated by software.
% Network Function Virtualisation (NFV) is arguably the
% most remarkable technology in this regard. NFV is the reaction
% of telco operators to their lack of agility and constant
% need for reliable infrastructures. NFV tries to provide
% the ability of dynamically deploying on-demand network services
% (e.g. a firewall, a router or a WAN accelerator, a new
% LAN or a VPN) or user-services (e.g. a database) where
% and when needed. Software Defined Networks SDN are one
% of the pillars needed for NFV, since some network services
% (e.g. creating new virtual networks on top of the physical
% infrastructure) can be done by software only. For instance,
% some gateways can be deployed as virtual machines and their
% traffic can be tightly controlled thanks to SDN capabilities
% in a local edge cloud, see Figure 1. The softwareisation of
% a classically hardware-driven business built around routers
% and servers where services got deployed will result in cheaper
% and more agile operations.
% A complementary approximation is proposed by Cisco
% with its first software-only version of the IOS wrapped in
% with a Linux distribution (IOx)3
% . The router itself becomes
% an SDN-enabled virtualisation infrastructure where
% NFV and application services are deployed close to the place
% where they are actually going to be used. But IO computing
% capabilities will still be limited (edge routers are not
% carrier grade after all).''

% according to [http://www.tmcnet.com/tmc/whitepapers/documents/whitepapers/2013/9378-bell-labs-metro-network-traffic-growth-an-architecture.pdf]4G LITE/EPC is supposed to be ready in 2017. It will expand the bandwidth of edge networks.


% % Overview of fog: [http://annals-csis.org/Volume_2/pliks/503.pdf]
% % [http://csjournals.com/IJITKM/PDF%208-2/13.%20Sagar.pdf]

% % Power aware load balancing for cloud computing [http://www.iaeng.org/publication/WCECS2011/WCECS2011_pp127-132.pdf]
% % hyvaa kamaa? [Raghava & Singh: Comparative Study on Load Balancing Techniques in Cloud Computing]

% % Power Aware Load Balancing for Cloud Computing [http://www.iaeng.org/publication/WCECS2011/WCECS2011_pp127-132.pdf]

\section{Virtualization}
\label{section:virtualization}
\todo[inline]{What is virtualization? Why is virtualization used? How does it change (complicate) the way things (i.e. routing etc) is done?}

Virtualization is an act of dividing physical computing resources, for example central processing units, memory, or storage, into a multiple execution environments.

Snapshot of a virtual machine describes the state of the machine at a certain point of time. Storing a snapshot of a virtual machine allows the machine to be shut and restarted again with possibly exact state.

Snapshot of a virtual machine can be migrated into another physical host.

\section{Stream Computing}

\todo[inline]{Internet of Things, fog computing}
\todo[inline]{From Von Neumann to Streams. Window-type view of the data. Real-time.}

\section{Distributed Computing}

\todo[inline]{Distributed computing is widely used e.g. in scientific computing}

\todo[inline]{write introduction to the backgroup chapter}
\todo[inline]{rename the background}

\section{Evolved Packet Core}
\label{section:evolved-packet-core}

The main components of Evolved Packet Core are
  - Home Subscriber Server (HSS)
    HSS contains the information about the network subscribers
  - Packet Data Network Gateway (P-Gw)
    P-GW works as the communicator between the EPC and the core networks (PDNs).
    uses SGi interface
    PDN's are identified by an access point name (APN).
  - Serving gateway (S-GW)
    the router which forwards data between the base station and P-GW
  - Mobility Management Entity (MME)
    signals messages and utilizes Home Subscriber Server (HSS) to control the high-level operations of the mobile
  - Policy Control and Charding Rules Function (PCRF)
    PCRF responsibility is to handle the policy control decisions, and charging functionality in the Policy Control Enforcement Function (PCEF) inside P-GW

% http://www.3gpp.org/technologies/keywords-acronyms/100-the-evolved-packet-core
% Introduction to Evolved Packet Core - Alcatel-Lucent

HSS:
Home Subscriber Server database has two main function. It contains information related to the network users and subscribers. Its second function is to provide support functions in mobility management, call and session setup, user authentication and access authorization. HSS is based on Home Location Register (HLR) and Authentication center (AuC).

% Simulation of LTE Signaling [http://www.dasconference.ro/cd2010/data/papers/B118.pdf]
% Cisco virtual EPC [http://www.cisco.com/c/en/us/solutions/service-provider/virtualized-packet-core/index.html] [https://www.cisco.com/c/dam/en/us/solutions/collateral/service-provider/mobile-internet/Cisco_LTE_Policy_Management_WP.pdf]
% Ericsson vEPC [http://www.ericsson.com/ourportfolio/products/evolved-packet-core?nav=productcategory004]
% random EPC thesis [http://ece.engineering.ualberta.ca/en/Graduate/~/media/ece/Graduate/Documents/GradDocs/Intro_to__Evolved_packet_core_network.pdf]
% good looking slides [http://www.cvt-dallas.org/Aug12-Sridhar.pdf]

% LTE QCI (QoS Class Identifier), as defined by 3GPP TS23.203

\subsection{EPC protocols}
\label{sec:epc-protocols}
S1AP [http://www.codenomicon.com/products/defensics/datasheets/s1ap-server-suite.html]:
S1AP is the control plane signalling protocol between E-UTRAN and Evolved Packet Core. S1AP uses S1-MME interface, which is located between eNodeB and the MME. S1AP functions can be divided into two categories: UE associated services and non UE associated services. UE associated services relate to a single UE, whereas the non UE associated services is comprised of all the other S1-MME interface related services.

S1AP is transferred over SCTP protocol, and is defined in TS 36.413.

S1AP/NAS:
S1AP protocol offers NAS signal transport, which enables transport functionality between UE and MME. The NAS protocol enables the mobility- and session management between the UE and MME. NAS is defined in the TS 24.301.

DIAMETER:
Diameter is the base protocol for authentication, authorization and accounting (AAA) in EPS, which can be extended to provide AAA services for different access technologies.

GTPv2:

PMIPv6:
PMIPv6 (Proxy Mobile IPv6) protocol enables IPv6 nodes to remain reachable while moving (physically?) around in the IPv6 Internet.

X2AP:
[http://www.eventhelix.com/lte/handover/LTE-X2-Handover-Messaging.pdf]
RAB - radio access bearer
GTP - GPS Tunneling protocol

\section{Distributed Computing}

\todo[inline]{Distributed computing is widely used e.g. in scientific computing}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis-hartikainen"
%%% End:
