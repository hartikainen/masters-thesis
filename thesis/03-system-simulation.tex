\chapter{System Performance Analysis and Simulation}
\label{chapter:system-performance-analysis-and-simulation}

In this thesis, we study and evaluate the performance of a distributed stream computing system. Our evaluations are based on the modeling and simulation techniques, while measurements are also needed to build the model. Similarly to many other packet processing system studies~\cite{cavium:2010:fundamentals}, we are mostly interested in the communication delays, namely throughput and latency, of the system.

This chapter introduces the methods used in our experiments. We will start by introducing the common performance analysis techniques and metrics, and then further examine the modeling and simulation techniques. After that, \todo[inline]{yadayada}

\section{Performance Analysis}
For almost every computer system -- whether it be a high performance application on the cloud~\cite{jackson:2010:HPCOC} or an army fuel-supply system~\cite{sabuncuoglu:2005:TAS} -- the performance is one of the most sought-after criterion. To achieve the highest performance for the lowest cost, different performance evaluation techniques are required at different system life cycle stages. The choice of evaluation criteria and techniques, used to evaluate the system performance, vary between systems. These two choices are discussed in the following subsections.~\cite{jain:1991:AOCSPA}

\subsection{Evaluation Techniques}
Performance evaluation can be done using various techniques. These techniques are generally divided into three categories: analytical modeling, simulation, and measurement. The former two techniques are based on symbolic models of the real-life system, whereas the measurements are done on the system itself. Analytical approaches use mathematical methods to solve the model, and simulation imitates the operation of the system by executing the model on a simulator~\cite{Banks:2010:DES}. Measurements are done by instrumenting the real system with various hardware and software tools.~\cite{jain:1991:AOCSPA}

No strict programmatic rules can be given to select the right technique. However, there are some considerations that can be used to guide the decisions: system life-cycle stage; available resources, such as time, money and tools; required level of accuracy; trade-off evaluation; and saleability.~\cite{jain:1991:AOCSPA}

The life-cycle stage of the system is often the first consideration. In early design stage, the evaluation is often done by using analytical methods or simulation, as it is impossible to measure a yet non-existing system. Measurements are, thus, often used for improving an existing systems.~\cite{jain:1991:AOCSPA}

Available resources also dictate the technique selection. Running the measurements and simulations are often more time consuming~\cite{Fujimoto:1990:PDE} than the analytic approach, and the required time can be hard to predict. They both also require special equipment and tools, which are expensive and need special skills to operate. The analytical methods are generally considered less time consuming and less expensive than measurement and simulation.~\cite{jain:1991:AOCSPA}

\todo[inline]{PDES stuff: parallelizing is hard - scaling simulation is non-trivial?}

The required level of accuracy should also be considered. For analytical models to be solvable, they often have to be very simple abstractions of the original system. Thus, the results of the analytical methods are often approximate. Simulations, like analytical methods, are abstract, but often much closer to the real system. Even measurements can produce results that do not agree with the actual system behavior. Generally measurements can be considered the most accurate, and analytical methods more inaccurate than the simulation. The accuracy of simulations and measurements can often be enhanced by spending more time and money to them.~\cite{jain:1991:AOCSPA}

Different evaluation techniques are often used together. Taking advantage of two or more methods simultaneously can be used to validate and verify the analysis results. On the other hand, different methods can be used to complement each other to enhance the analysis process.~\cite{jain:1991:AOCSPA}

\todo[inline]{performance analysis like art?}

\subsection{Performance Metrics}
Every performance study needs a set of performance criteria or metrics, which vary with the service provided by the system. Service requests made to the system produce different outcomes: the system either performs the service -- correctly or incorrectly -- or refuses to perform it. The metrics associated with these outcomes are called speed, reliability, and availability, respectively.~\cite{jain:1991:AOCSPA}

When the service result is correct, the performance metrics are used to measure the responsiveness, productivity and utilization of the system. For example, in a network packet processing system, the responsiveness could be measured as the packet response time, the productivity as the throughput, and the utilization as the percentage of time the cores are busy~\cite{cavium:2010:fundamentals}.~\cite{jain:1991:AOCSPA}

If the service result is incorrect, the metrics describe the probabilities of the error. For example, how probably an unintentional packet drops or out-of-orderings occur. When the system fails to perform requested service, it is helpful the classify the different causes of failure, and determine the probability and the duration for each.~\cite{jain:1991:AOCSPA}

It should be noted that many systems provide multiple services, and the number of metrics can be large. Also, different evaluation techniques provide different metrics at different times of the service. For example, some simulators allow white-box-like view of the system states during the simulation, whereas, with analytical methods, the details of the system are often unavailable.~\cite{jain:1991:AOCSPA}~\cite{TODO: find reference for the white-box black-box stuff}

\section{Simulation}
This sections describes the basic principles related to simulation. Note that, despite the most of the examples in this chapter being about computing, simulation is widely used in several other contexts, even outside science. The concepts described below, e.g. entities, attributes, or activities, to name a few, have different realizations from system to system.

Simulation is an artificial imitation of the operation of a real-life system over time. The system behavior is studied by developing a simulation model, based on a set assumptions concerning the characteristics and functions of the system. The assumptions are presented in mathematical, logical, and symbolic relationships between the objects of interest of the system. An artificial operation history is generated by executing the simulation model, generally on a simulator program, with respect to system input and time. Data are collected from the simulation similarly as if the real system was being measured.

\subsection{System components and environment}
\label{sec:syst-comp-envir}

In simulation, a \emph{system} is defined to be a set of objects that work together, in regular interaction or interdependence, to accomplish some goal or purpose. Every system is a subsystem of broader \emph{system environment}, whose changes can affect the system. For every simulation study, a \emph{boundary} between the system and its environment must be set.~\cite{Banks:2010:DES}

For example, computer systems are often enormously complex. The complexity is managed by designing them as a hierarchy of smaller subsystems, and combining them with compatible interfaces. In a study of a network processing system, such as~\cite{cavium OCTEON}, the higher level system can be viewed to consist of several processors, auxiliary memory, memory controllers, and other smaller subsystems. These subsystems can further be viewed as a set of smaller subsystems of subsystems: the processor has several processing cores, each core consists of different functional units, and each functional unit consists of logical circuitry.~\cite{Banks:2010:DES}

The objects of interest in a system are called \emph{entities}, which are associated with a set \emph{attributes}. An \emph{activity} is a specified length time period. A system \emph{state} completely describes the system at any given time of a specific study. The system state might be changed by an immediate occurrences called \emph{events}. The events affecting the system are divided into two groups by their source: \emph{endogenous} events occur within the system under study, and \emph{exogenous} events occur in the system environment.~\cite{Banks:2010:DES}

Continuing with the above example, each of the mentioned components can be seen as the entities of the system. There are several activities and events at different levels of the system. At the higher level, these can be seen as the packets flowing through the system: writes and reads to the memory, execution on different processors, and queueing for the processor time. At a lower level, these could be the computation done by the logical units or the flipping of the transistors' state.

Systems can be divided into two categories as per the type of their state change: discrete systems and continuous systems. In a \emph{discrete system}, the system's state changes only at a discrete time points, and in a \emph{continuous system}, the state change is continuous over time. In practice, almost every system is a combination of both continuous and discrete changes. These systems are often classified by the dominant type.~\cite{Banks:2010:DES}

\subsection{Simulation Model}
\label{sec:simulation-model}

Simulation \emph{model} is a representation of either hypothetical or real-life system under study. By its definition~\cite{Encyclopedia of computer science}, the model should be a simplified representation of the original system. In simulation, it should represent the studied system with enough detail to provide relevant conclusions and, at the same time, only consider those details that affect the investigated problem. The decision between the level of accuracy and abstraction usually requires knowledge of the system under modeling.~\cite{Banks:2010:DES}

Like with the system representation, the basic building blocks of the simulation model are entities, attributes, activities and events. The model does not necessarily contain the exact replica of the components of the system, but rather simplified components that represent the system with enough detail.~\cite{Banks:2010:DES}

In the example study of network processing unit, the likely goals would be to determine the packet throughput and latency of the system. In that case, the system could be modeled with sufficient accuracy by omitting all the lower level details of the CPU. On the other hand, a detailed performance study of a specific CPU might require even the minute details of the functional units or logical circuitry.~\cite{TODO: find some simulation example for this}

The models may be categorized as being static or dynamic, and deterministic or stochastic. Static simulation models represent a steady-state time-invariant systems, whereas the dynamic simulation models represent systems as time-variant. Deterministic models contain no random variables, meaning that for the known set of inputs, the simulation result will be a unique set of outputs. Stochastic models on the other hand include random variables, thus leading to a random set of outputs.~\cite{Banks:2010:DES}

Simulation models are further divided into discrete and continuous, analogously with the discrete and continuous systems described in Section \ref{sec:syst-comp-envir}. However, it is possible to model continuous systems with discrete models, and vice versa. Just as the real-life systems, the simulation models can be a mix of both continuous and discrete models.~\cite{Banks:2010:DES}

Three different simulation advancement designs are presented in~\cite{peros:2009:simulation}: event-advance, unit-time advance and activity based. In event-advance design the system state changes only when the event occurs. Thus the system state advances from snapshot to snapshot, meaning that the state is unchanged between two successive events. In unit-time advance design, the master clock is advanced in fixed time increments. Activity based design is a continuous design method, which models the system as a set of conditions that determine when the activities start or stop.~\cite{peros:2009:simulation}

\subsection{Monitoring}

To make conclusion about simulation, the information about the simulation system needs to be gathered. Similarily to the measurement based techniques, the system is instrumented and the data are saved during the execution.

There are two generally used approaches for gathering simulation metrics: trace-based and on-the-fly. The tracing approaches produce raw data from the execution, which are then often post-processed in suitable way. In on-the-fly approaches, the simulator program aggregates the data during the simulation, thus reducing the amount of output.\todo{cite, jain?}

\todo[inline]{add the resource network stuff somewhere?}

\section{Simulator Software}
\todo[inline]{describe available simulator software here?}

\todo[inline]{Rsim: More detailed hardware simulation, with focus on ILP effects of processors
      Detailed shared-memory multiprocessor models
      They show that the previous simple-processor-models cannot demonstrate the complicated dynamically scheduled processors with sufficient detail.}

\todo[inline]{Simics: ``Reliable performance estimates require simulation of the full system''
        Their simulator includes device models accurate enough to run unmodified operating system kernels, firmware and device driver.
        They simulate a network of heterogeneous computers.
        Commercial product
        Two dimensional classification:
          scope (what is modeled),
          abstraction level (level at which the system is modeled)
            two perspectives: functional, timing (performance)}

\todo[inline]{SimpleScalar:
  Hardware development is often accelerated by software simulations of the hardware under development.
  Basic idea: Implement a software model of the hardware, stress then with appropriate workload
  Gains: The availability of the simulation model is much faster than building the hardware. Thus the hardware can be tested faster. Shorter time to market, cut down development costs.
  Simulation modeling is driven by three factors: performance, flexibility, detail
    Performance measures the system's ability to process workload
    Flexibility reflects the ability to modify the models, enabling measurements of different designs
    Detail means the level of abstraction the model captures from the actual system
    -> Getting all three is hard, practically impossible.
    -> trade offs}

% In discrete-event simulation models the system's state at each point of discrete times. The system is modeled in terms of the entities flowing through the system utilizing other entities that represent the system resources.~\cite{Banks:2010:DES}

% DES is one way of progressing the simulation

% The idea of the general model is expanded with queues, event lists, delay, clock.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis-hartikainen"
%%% End:
